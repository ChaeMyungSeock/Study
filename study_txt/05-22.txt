
model.add(Dense(10,Reshape((3,),input_shape=(3,1)))
layer 단에서도 Reshape가 가능함 근데 2차원에서 1차원으로 격하시키고 싶은데 왠지모르게 안됨

데이터가 부족할 수록 그리고 차원이 간단할 수록 데이터의 구조가 간단할 수록 dense의 구조가 데이터를 더 잘 파악함

scikit-learn에서는 다음과 같은 스케일링 클래스를 제공한다.

StandardScaler(X): 평균이 0과 표준편차가 1이 되도록 변환.
- 기존 변수에 범위를 정규 분포로 변환
- (x - x의 평균값)/ (x의 표준편차)
- 데이터의 최소, 최대 값을 모를 경우

[ 각 피처의 평균을 0, 분산을 1로 변경합니다. 모든 특성들이 같은 스케일을 갖게 됩니다. 평균을 
제거하고 데이터를 단위 분산으로 조정합니다. 그러나 이상치가 있다면 평균과 표준편차에 영향을
미쳐 변환된 데이터의 확산은 매우 달라지게 됩니다. 따라서 이상치(Outlier)가 있는 경우 균형 잡힌
척도를 보장할 수 없게 됩니다. ]

데이터 : 1 2 3 4 10
평균 : 4
편차 : 3 2 1 0 -6
분산 : 9 4 1 0 36 =>  (9+4+1+0+36) / 5 = 10
표준편차 = route(10)

RobustScaler(X): 중앙값(median)이 0, IQR(interquartile range)이 1이 되도록 변환.
- StandardScaler에 의한 표준화보다 동일한 값을 더 넓게 분포
- 이상치(outlier)를 포함하는 데이터를 표준화하는 경우

[ 모든 특성들이 같은 크기를 갖는다는 점에서 Standard Scaler와 비슷하지만, 평균과 분산 대신 
Median과 IQR(interquartile range)을 사용하며, 이상치(Outlier)의 영향을 최소화 합니다. 
StandardScaler와 비교해보면 표준화 후 동일한 값을 더 넓게 분포 시키고 있음을 확인 할 수 
있습니다. ( IQR = Q3 - Q1 : 25% ~ 75% 타일의 값을 다룬다. ] 

MinMaxScaler(X): 최대값이 각각 1, 최소값이 0이 되도록 변환 => 정규화
- 데이터를 0~1사이의 값으로 변환
- (x - x의 최소값) / (x의 최대값 - 최소값)
- 데이터의 최소, 최대값을 알 경우 사용

[ 모든 피처가 0과 1사이에 값을 가집니다. 최대값이 1이 되고 최소값이 0이 되도록 스케일링 합니다.
데이터가 2차원 셋일 경우, x축과 y축 값 모두 0과 1사이의 값을 가집니다. 상치가 있는 경우 변환된 
값이 매우 좁은 범위로 압축될 수 있습니다. MinMax 역시 이상치(Outlier)의 존재에 민감합니다.
MaxAbs Scaler라는 것도 있는데, 최대 절대값과 0이 각 1,0이 되도록 하여 양수 데이터로만 구성되게 
스케일링하는 기법 ]


MaxAbsScaler(X): 0을 기준으로 절대값이 가장 큰 수가 1또는 -1이 되도록 변환
- 데이터를 0-1 사이의 값으로 변환

Normalizer

[ Standard Scaler, Robust Scaler, MinMax Scaler는 각 칼럼의 통계치를 이용한다면, Normalizer는
각 로우마다 정규화 됩니다. Normalizer는 유클리드 거리가 1이 되도록 데이터를 조정합니다.
Normalize를 하게 되면 Spherical contour(구형 윤곽)을 갖게 되는데, 이렇게 하면 좀 더 빠르게 
학습할 수 있고 과대적합 확률을 낮출 수 있습니다.


스케일링(Scaling)이란
 - 데이터 전처리 과정의 하나

- 데이터의 값이 너무 크거나 혹은 작은 경우에 모델 알고리즘 학습과정에서 0으로 수렴하거나 
  무한으로 발산해 버릴 수 있는 것을 방지하거나, 예측 값이 범위를 벗어나는 입력데이터의 값에
  더 큰 영향을 받는 것을 방지합니다

- 스케일링을 통해 다타원의 값들을 비교 분석하기 쉽게 만들어 줍니다,

- 자료의 overflow나 underflow를 방지하고 최적화 과정에서 안정성 및 수렴 속도를 향상시킵니다.

예를 들어 키와 몸무게가 입력 데이터로 주어지고 라벨데이터는 100m달리기에 걸리는 시간을 예측
한다고 하면, 키와 몸무게는 서로 범위, Unit이 다르기 때문에 더 큰 값을 가진 키 값이 결과 값에 더
큰 영향을 미치는 것을 방지하기 위해 키와 몸무게 데이터의 유닛을 맞춰주는 작업을 해야합니다. 
그럴때 사용하는 것이 바로 Scaling 입니다.

======================================

model.add(LSTM(10, input_length = 3, input_dim = 1, return_sequences = True))
model.add(LSTM(10,return_sequences =False))
lstm_1 (LSTM)                (None, 3, 10)             480
_________________________________________________________________
lstm_2 (LSTM)                (None, 10)                840

# Dense layer는 행열 2차원만 받고 output 또한 2차원 하지만 LSTM은 (행, 열, 피쳐) 3차원을 필요로 
함으로 return_sequences를 True로 리턴 받아서 3차원으로 확장시킨다 return_sequences의 디폴트값
은 False다.

# from keras.layers import Flatten
# model.add(Flatten()) => 데이터가 몇 차원이든 2차원으로 바꿔줘서 dense에서도 연산이 가능한 
  모델로 변환


# **기본적으로 단순한 데이터의 경우 Dense layer의 사용시 깊은 layer의 연산을 지양하는것이 결과값을 
   예측하는데 있어서 더 유리하다. **

from sklearn.preprocessing import MinMaxScaler ,StandardScaler
# scaler = MinMaxScaler()
scaler = StandardScaler()
scaler.fit(x) # => fit 실행하다 MinMaxScaler를 실행해라
x=scaler.transform(x) # 실행한 MinMaxScaler를 변환해줘라
x_predict = scaler.transform(x_predict)

sklearn에 들어가있는 클래스로 MinMaxScaler, StandardScaler => 설명은 위에 참조
데이터를 일정 범위내에 분포화 시켜서 데이터 예측을 더 쉽게 할 수 있음, 훈련 데이터를 분포화 해줬으면
x_predict값 또한 스케일 해줘야 함 (왜냐하면 x_predict값은 범위 내에 있기 때문) 그럴경우 y값은 왜 
분포화 시켜주지 않냐라고 궁금해 할 수 있는데 이는 x와 y값이 인덱스끼리 매칭된다고 생각하는것이 편하다
즉 분포화가 되더라도 일정 비율을 가지고 분포화 되므로 y값 매칭에는 큰 문제가 없음
 


a = np.array(range(1,11))
size = 5
def split_x(seq, size):
    aaa=[]
   # @aaa는 리스트라 선언 
    for i in range(len(a) - size + 1 ): 
    # @for문을 돌릴거야 i은 0부터 [ a리스트의 길이 - size(위에 5라고 선언) +1 ] 까지
        subset = a[i : (i+size)] 
        # @subset이라는 변수에 a리스트의 a[i] 부터 a[i+size-1]까지 넣어줄게
        # print(f"{ i+1 }: { subset } ")
        aaa.append(subset)
        # @aaa라는 리스트에 subset을 추가할거야
        # aaa.append([item for item in subset])
    print(type(aaa)) 
    # @aaa의 타입을 프린트
    return np.array(aaa) 
    # @리스트 aaa를 np.array로 반환






